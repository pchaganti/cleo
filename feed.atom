<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>urn:2025-03-30T22:22:03.402Z</id>
    <title>pchaganti | cleo</title>
    <updated>2025-03-30T22:22:03.402Z</updated>
    <generator>osmosfeed 1.15.1</generator>
    <link rel="alternate" href="index.html"/>
    <entry>
        <title type="html"><![CDATA[Links 3/30/2025]]></title>
        <id>https://www.nakedcapitalism.com/?p=289799</id>
        <link href="https://www.nakedcapitalism.com/2025/03/links-3-30-2025.html"/>
        <updated>2025-03-30T10:55:20.000Z</updated>
        <summary type="html"><![CDATA[Our strategic daily links: auto patina, shrinking ice, Chinese SST, Cuban doctors, EU farmers, Gaza horrors, Ukraine war, student deportations, vanishing privacy, flushing fentanyl, Greenland acquisitiveness, DOGEy , moves, immigration troubles, plunging market, hilarious movie trailers, and wretched excess]]></summary>
        <author>
            <name>Haig Hovaness</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Alignment Between the US and Türkiye Means Trouble]]></title>
        <id>https://www.nakedcapitalism.com/?p=289825</id>
        <link href="https://www.nakedcapitalism.com/2025/03/increasing-alignment-between-the-us-and-turkiye-means-trouble.html"/>
        <updated>2025-03-30T10:00:18.000Z</updated>
        <summary type="html"><![CDATA[Washington and Ankara are getting mighty cozy as the project to encircle Iran picks up steam.]]></summary>
        <author>
            <name>Conor Gallagher</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trump and Musk Are Making It Harder for the IRS to Catch Wealthy Tax Cheats]]></title>
        <id>https://www.nakedcapitalism.com/?p=289834</id>
        <link href="https://www.nakedcapitalism.com/2025/03/trump-and-musk-are-making-it-harder-for-the-irs-to-catch-wealthy-tax-cheats.html"/>
        <updated>2025-03-30T09:00:31.000Z</updated>
        <summary type="html"><![CDATA[The DOGE dismantlement/takeover of the IRS continues apace.]]></summary>
        <author>
            <name>Conor Gallagher</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[For this computer scientist, MIT Open Learning was the start of a life-changing journey]]></title>
        <id>https://news.mit.edu/2025/mit-open-learning-resources-life-changing-journey-0330</id>
        <link href="https://news.mit.edu/2025/mit-open-learning-resources-life-changing-journey-0330"/>
        <updated>2025-03-30T04:00:00.000Z</updated>
        <summary type="html"><![CDATA[Ana Trišović, who studies the democratization of AI, reflects on a career path that she began as a student downloading free MIT resources in Serbia.]]></summary>
        <author>
            <name>Lauren Rebecca Thacker | MIT Open Learning</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Links 3/29/2025]]></title>
        <id>https://www.nakedcapitalism.com/?p=289782</id>
        <link href="https://www.nakedcapitalism.com/2025/03/links-3-29-2025.html"/>
        <updated>2025-03-29T10:55:02.000Z</updated>
        <summary type="html"><![CDATA[Our strategic daily links: CDC hides measles warning, Gates food fail, Myanmar quake, Canada divorces US, European strategic minerals scramble, moar stealth bombers to Diego Garcia, draconian US "minerals deal" a ruse to end support? Trump stalled on CFPB + some law firms, DOGE scheme to kill Social Security]]></summary>
        <author>
            <name>Yves Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Book Review: The Dazzling Complexity of the Polar Realm]]></title>
        <id>https://www.nakedcapitalism.com/?p=289793</id>
        <link href="https://www.nakedcapitalism.com/2025/03/book-review-the-dazzling-complexity-of-the-polar-realm.html"/>
        <updated>2025-03-29T10:28:10.000Z</updated>
        <summary type="html"><![CDATA[In “Ends of the Earth,” Neil Shubin travels north and south to explore the frontiers of polar science and history.]]></summary>
        <author>
            <name>Yves Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Worker-Owned News Outlets Are Changing the Media Industry]]></title>
        <id>https://www.nakedcapitalism.com/?p=289791</id>
        <link href="https://www.nakedcapitalism.com/2025/03/how-worker-owned-news-outlets-are-changing-the-media-industry.html"/>
        <updated>2025-03-29T10:07:33.000Z</updated>
        <summary type="html"><![CDATA[For growing numbers of media companies, employee ownership offers journalistic freedom and job stability.]]></summary>
        <author>
            <name>Yves Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Daily Reading List – March 28, 2025 (#522)]]></title>
        <id>http://seroter.com/?p=21789</id>
        <link href="https://seroter.com/2025/03/28/daily-reading-list-march-28-2025-522/"/>
        <updated>2025-03-28T23:26:06.000Z</updated>
        <summary type="html"><![CDATA[Today's links look at why you should keep learning to code, the role of devs in agentic coding, and why IDPs are an important part of your developer strategy.]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coffee Break: Covid Denialism Is Not a Cure; Updates on the War on Cancer and Medicare]]></title>
        <id>https://www.nakedcapitalism.com/?p=289771</id>
        <link href="https://www.nakedcapitalism.com/2025/03/coffee-break-covid-denialism-is-not-a-cure-updates-on-the-war-on-cancer-and-medicare.html"/>
        <updated>2025-03-28T18:00:21.000Z</updated>
        <summary type="html"><![CDATA[Covid and cancer are not going away soon, but if Dr. Oz has his way, Medicare might.]]></summary>
        <author>
            <name>KLG</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quoting Colin Fraser]]></title>
        <id>https://simonwillison.net/2025/Mar/28/colin-fraser/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/28/colin-fraser/#atom-everything"/>
        <updated>2025-03-28T16:15:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote cite="https://twitter.com/colin_fraser/status/1905158970337083676"><p>Slop is about collapsing to the mode. It’s about information heat death. It’s lukewarm emptiness. It’s ten million approximately identical cartoon selfies that no one will ever recall in detail because none of the details matter.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/colin_fraser/status/1905158970337083676">Colin Fraser</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/slop">slop</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Next American Constitution]]></title>
        <id>https://www.nakedcapitalism.com/?p=289758</id>
        <link href="https://www.nakedcapitalism.com/2025/03/the-next-american-constitution.html"/>
        <updated>2025-03-28T14:21:56.000Z</updated>
        <summary type="html"><![CDATA[Trump's Constitution-trampling has historical predecents. Where might we be headed?]]></summary>
        <author>
            <name>Yves Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[As AI’s Power Grows, So Does Our Workday]]></title>
        <id>https://www.nakedcapitalism.com/?p=289770</id>
        <link href="https://www.nakedcapitalism.com/2025/03/as-ais-power-grows-so-does-our-workday.html"/>
        <updated>2025-03-28T13:55:40.000Z</updated>
        <summary type="html"><![CDATA[Staffers uing AI are putting in more hours relative to non-AI-deploying positions. So who is working for whom?]]></summary>
        <author>
            <name>Yves Smith</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Owen Hatherley: &ldquo;Mislaid Plans&rdquo;]]></title>
        <id>https://newleftreview.org/sidecar/posts/mislaid-plans</id>
        <link href="https://newleftreview.org/sidecar/posts/mislaid-plans"/>
        <updated>2025-03-28T13:33:26.000Z</updated>
        <summary type="html"><![CDATA[On the Manic Street Preachers.]]></summary>
        <author>
            <name>New Left Review Sidecar recent posts</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Links 3/28/2025]]></title>
        <id>https://www.nakedcapitalism.com/?p=289737</id>
        <link href="https://www.nakedcapitalism.com/2025/03/links-3-28-2025.html"/>
        <updated>2025-03-28T10:55:12.000Z</updated>
        <author>
            <name>Conor Gallagher</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete JSON Pretty Printer]]></title>
        <id>https://simonwillison.net/2025/Mar/28/incomplete-json-pretty-printer/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/28/incomplete-json-pretty-printer/#atom-everything"/>
        <updated>2025-03-28T00:18:43.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://tools.simonwillison.net/incomplete-json-printer">Incomplete JSON Pretty Printer</a></strong></p>
Every now and then a log file or a tool I'm using will spit out a bunch of JSON that terminates unexpectedly, meaning I can't copy it into a text editor and pretty-print it to see what's going on.</p>
<p>The other day I got frustrated with this and had the then-new GPT-4.5 build me a pretty-printer that didn't mind incomplete JSON, using an OpenAI Canvas. Here's <a href="https://chatgpt.com/share/67dd9d55-7f70-8006-b55d-72730f60ddbe">the chat</a> and here's <a href="https://chatgpt.com/canvas/shared/67e5e9b3f7bc8191b2306a123c9d328f">the resulting interactive</a>.</p>
<p>I spotted a bug with the way it indented code today so I pasted it into Claude 3.7 Sonnet Thinking mode and had it make a bunch of improvements - <a href="https://claude.ai/share/22dc4b58-e8c4-44a4-9650-a37d21513b8d">full transcript here</a>. Here's the <a href="https://github.com/simonw/tools/blob/main/incomplete-json-printer.html">finished code</a>.</p>
<p><img alt="Animated GIF demo - as I type JSON it is pretty printed below, at the end I click the Load Pelican Example button." src="https://static.simonwillison.net/static/2025/pretty-print-json.gif" /></p>
<p>In many ways this is a perfect example of <a href="https://simonwillison.net/2025/Mar/19/vibe-coding/">vibe coding </a> in action. At no point did I look at a <em>single line</em> of code that either of the LLMs had written for me. I honestly don't care how this thing works: it could not be lower stakes for me, the worst a bug could do is show me poorly formatted incomplete JSON.</p>
<p>I was vaguely aware that some kind of state machine style parser would be needed, because you can't parse incomplete JSON with a regular JSON parser. Building simple parsers is the kind of thing LLMs are surprisingly good at, and also the kind of thing I don't want to take on for a trivial project.</p>
<p>At one point I told Claude "Try using your code execution tool to check your logic", because I happen to know Claude can write and then execute JavaScript independently of using it for artifacts. That helped it out a bunch.</p>
<p>I later dropped in the following:</p>
<blockquote>
<p><code>modify the tool to work better on mobile screens and generally look a bit nicer - and remove the pretty print JSON button, it should update any time the input text is changed. Also add a "copy to clipboard" button next to the results. And add a button that says "example" which adds a longer incomplete example to demonstrate the tool, make that example pelican themed.</code></p>
</blockquote>
<p>It's fun being able to say "generally look a bit nicer" and get a perfectly acceptable result!


    <p>Tags: <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/tools">tools</a>, <a href="https://simonwillison.net/tags/json">json</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/vibe-coding">vibe-coding</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quoting Nelson Minar]]></title>
        <id>https://simonwillison.net/2025/Mar/28/nelson-minar/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/28/nelson-minar/#atom-everything"/>
        <updated>2025-03-28T00:08:01.000Z</updated>
        <summary type="html"><![CDATA[<blockquote cite="https://lobste.rs/s/aygeaq/atom_vs_rss_2013#c_mxxurc"><p>I was there at the first Atom meeting at the Google offices. We meant so well! And I think the basic publishing spec is good, certainly better technically than the pastiche of different things called RSS.</p>
<p>Alas, a bunch of things then went wrong. Feeds started losing market share. Facebook started doing something useful and interesting that ultimately replaced blog feeds in open formats. The Atom vs RSS spec was at best irrelevant to most people (even programmers) and at worst a confusing market-damaging thing. The XML namespaces in Atom made everyone annoyed. Also there was some confusing “Atom API” for publishing that diluted Atom’s mindshare for feeds.</p></blockquote>
<p class="cite">&mdash; <a href="https://lobste.rs/s/aygeaq/atom_vs_rss_2013#c_mxxurc">Nelson Minar</a>, Comment on lobste.rs</p>

    <p>Tags: <a href="https://simonwillison.net/tags/atom">atom</a>, <a href="https://simonwillison.net/tags/syndication">syndication</a>, <a href="https://simonwillison.net/tags/nelson-minar">nelson-minar</a>, <a href="https://simonwillison.net/tags/rss">rss</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Daily Reading List – March 27, 2025 (#521)]]></title>
        <id>http://seroter.com/?p=21772</id>
        <link href="https://seroter.com/2025/03/27/daily-reading-list-march-27-2025-521/"/>
        <updated>2025-03-27T23:08:36.000Z</updated>
        <summary type="html"><![CDATA[Today's links look at what keeps tech leaders up at night, how to serve open LLMs with one CLI call, and specific skills for doing technical writing.]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracing the thoughts of a large language model]]></title>
        <id>https://simonwillison.net/2025/Mar/27/tracing-the-thoughts-of-a-large-language-model/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/27/tracing-the-thoughts-of-a-large-language-model/#atom-everything"/>
        <updated>2025-03-27T21:51:24.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://www.anthropic.com/research/tracing-thoughts-language-model">Tracing the thoughts of a large language model</a></strong></p>
In a follow-up to the research that brought us the <a href="https://simonwillison.net/2024/May/24/golden-gate-claude/">delightful Golden Gate Claude</a> last year, Anthropic have published two new papers about LLM interpretability:</p>
<ul>
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html">Circuit Tracing: Revealing Computational Graphs in Language Models </a> extends last year's interpretable features into <a href="https://transformer-circuits.pub/2025/attribution-graphs/methods.html#graphs">attribution graphs</a>, which can "trace the chain of intermediate steps that a model uses to transform a specific input prompt into an output response".</li>
<li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model</a> uses that methodology to investigate Claude 3.5 Haiku in a bunch of different ways. <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-multilingual">Multilingual Circuits</a> for example shows that the same prompt in three different languages uses similar circuits for each one, hinting at an intriguing level of generalization.</li>
</ul>
<p>To my own personal delight, neither of these papers are published as PDFs. They're both presented as glorious mobile friendly HTML pages with linkable sections and even some inline interactive diagrams. More of this please!</p>
<p><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-multilingual"><img alt="Screenshot of a multilingual language model visualization showing antonym prediction across three languages. Left panel shows English with prompt &quot;The opposite of 'small' is'&quot; predicting &quot;large&quot;. Middle panel shows Chinese prompt &quot;小&quot;的反义词是&quot; predicting &quot;大 (zh: big)&quot;. Right panel shows French prompt &quot;Le contraire de &quot;petit&quot; est&quot; predicting &quot;grand (fr: big)&quot;. Above shows activation analysis with token predictions and highlighted instances of &quot;contraire&quot; in French text." src="https://static.simonwillison.net/static/2025/anthropic-diagrams.jpg" /></a>


    <p>Tags: <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/pdf">pdf</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/interpretability">interpretability</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPT-4o got another update in ChatGPT]]></title>
        <id>https://simonwillison.net/2025/Mar/27/gpt-4o-update/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/27/gpt-4o-update/#atom-everything"/>
        <updated>2025-03-27T21:32:40.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://twitter.com/OpenAI/status/1905331956856050135">GPT-4o got another update in ChatGPT</a></strong></p>
This is a somewhat frustrating way to announce a new model. @OpenAI on Twitter just now:</p>
<blockquote>
<p>GPT-4o got an another update in ChatGPT!</p>
<p>What's different?</p>
<ul>
<li>Better at following detailed instructions, especially prompts containing multiple requests</li>
<li>Improved capability to tackle complex technical and coding problems</li>
<li>Improved intuition and creativity</li>
<li>Fewer emojis 🙃</li>
</ul>
</blockquote>
<p>This sounds like a significant upgrade to GPT-4o, albeit one where the release notes are limited to a single tweet.</p>
<p>ChatGPT-4o-latest (2025-0-26) just hit second place on <a href="https://lmarena.ai/?leaderboard">the LM Arena leaderboard</a>, behind only Gemini 2.5, so this really is an update worth knowing about.</p>
<p>The @OpenAIDevelopers account <a href="https://twitter.com/OpenAIDevs/status/1905335104211185999">confirmed</a> that this is also now available in their API:</p>
<blockquote>
<p><code>chatgpt-4o-latest</code> is now updated in the API, but stay tuned—we plan to bring these improvements to a dated model in the API in the coming weeks.</p>
</blockquote>
<p>I <a href="https://simonwillison.net/2025/Feb/17/llm/#chatgpt-4o-latest">wrote about chatgpt-4o-latest</a> last month - it's a model alias in the OpenAI API which provides access to the model used for ChatGPT, available since August 2024. It's priced at $5/million input and $15/million output - a step up from regular GPT-4o's $2.50/$10.</p>
<p>I'm glad they're going to make these changes available as a dated model release - the <code>chatgpt-4o-latest</code> alias is risky to build software against due to its tendency to change without warning.</p>
<p>A more appropriate place for this announcement would be the <a href="https://platform.openai.com/docs/changelog">OpenAI Platform Changelog</a>, but that's not had an update since the release of their new audio models on March 20th.


    <p>Tags: <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thoughts on setting policy for new AI capabilities]]></title>
        <id>https://simonwillison.net/2025/Mar/27/ai-policy/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/27/ai-policy/#atom-everything"/>
        <updated>2025-03-27T21:22:29.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://reservoirsamples.substack.com/p/thoughts-on-setting-policy-for-new">Thoughts on setting policy for new AI capabilities</a></strong></p>
Joanne Jang leads model behavior at OpenAI. Their release of GPT-4o image generation included some notable relaxation of OpenAI's policies concerning acceptable usage - I <a href="https://simonwillison.net/2025/Mar/25/introducing-4o-image-generation/">noted some of those</a> the other day.</p>
<p>Joanne summarizes these changes like so:</p>
<blockquote>
<p>tl;dr we’re shifting from blanket refusals in sensitive areas to a more precise approach focused on preventing real-world harm. The goal is to embrace humility: recognizing how much we don't know, and positioning ourselves to adapt as we learn.</p>
</blockquote>
<p>This point in particular resonated with me:</p>
<blockquote>
<ul>
<li><strong>Trusting user creativity over our own assumptions</strong>. AI lab employees should not be the arbiters of what people should and shouldn’t be allowed to create.</li>
</ul>
</blockquote>
<p>A couple of years ago when OpenAI were the only AI lab with models that were worth spending time with it really did feel that San Francisco cultural values (which I relate to myself) were being pushed on the entire world. That cultural hegemony has been broken now by the increasing pool of global organizations that can produce models, but it's still reassuring to see the leading AI lab relaxing its approach here.


    <p>Tags: <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nomic Embed Code: A State-of-the-Art Code Retriever]]></title>
        <id>https://simonwillison.net/2025/Mar/27/nomic-embed-code/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/27/nomic-embed-code/#atom-everything"/>
        <updated>2025-03-27T20:03:56.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://www.nomic.ai/blog/posts/introducing-state-of-the-art-nomic-embed-code">Nomic Embed Code: A State-of-the-Art Code Retriever</a></strong></p>
Nomic have released a new embedding model that specializes in code, based on their CoRNStack "large-scale high-quality training dataset specifically curated for code retrieval".</p>
<p>The <a href="https://huggingface.co/nomic-ai/nomic-embed-code">nomic-embed-code</a> model is pretty large - 26.35GB - but the announcement also mentioned a much smaller model (released 5 months ago) called <a href="https://huggingface.co/nomic-ai/CodeRankEmbed">CodeRankEmbed</a> which is just 521.60MB.</p>
<p>I missed that when it first came out, so I decided to give it a try using my <a href="https://github.com/simonw/llm-sentence-transformers">llm-sentence-transformers</a> plugin for <a href="https://llm.datasette.io/">LLM</a>.</p>
<pre><code>llm install llm-sentence-transformers
llm sentence-transformers register nomic-ai/CodeRankEmbed --trust-remote-code
</code></pre>
<p>Now I can run the model like this:</p>
<pre><code>llm embed -m sentence-transformers/nomic-ai/CodeRankEmbed -c 'hello'
</code></pre>
<p>This outputs an array of 768 numbers, starting <code>[1.4794224500656128, -0.474479079246521, ...</code>.</p>
<p>Where this gets fun is combining it with my <a href="https://simonwillison.net/2023/Jun/18/symbex/">Symbex tool</a> to create and then search embeddings for functions in a codebase.</p>
<p>I created an index for my LLM codebase like this:</p>
<pre><code>cd llm
symbex '*' '*.*' --nl &gt; code.txt
</code></pre>
<p>This creates a newline-separated JSON file of all of the functions (from <code>'*'</code>) and methods (from <code>'*.*'</code>) in the current directory - you can <a href="https://gist.github.com/simonw/ac45c6638ea87942383e97c5cf69ae09">see that here</a>.</p>
<p>Then I fed that into the <a href="https://llm.datasette.io/en/stable/embeddings/cli.html#llm-embed-multi">llm embed-multi</a> command like this:</p>
<pre><code>llm embed-multi \
  -d code.db \
  -m sentence-transformers/nomic-ai/CodeRankEmbed \
  code code.txt \
  --format nl \
  --store \
  --batch-size 10
</code></pre>
<p>I found the <code>--batch-size</code> was needed to prevent it from crashing with an error. </p>
<p>The above command creates a collection called <code>code</code> in a SQLite database called <code>code.db</code>.</p>
<p>Having run this command I can search for functions that match a specific search term in that <code>code</code> collection like this:</p>
<pre><code>llm similar code -d code.db \
  -c 'Represent this query for searching relevant code: install a plugin' | jq
</code></pre>
<p>That <code>"Represent this query for searching relevant code: "</code> prefix is required by the model. I pipe it through <code>jq</code> to make it a little more readable, which gives me <a href="https://gist.github.com/simonw/fdc1b48b20a99714200f5d3970b1dff4">these results</a>.</p>
<p>This <code>jq</code> recipe makes for a better output:</p>
<pre><code>llm similar code -d code.db \
  -c 'Represent this query for searching relevant code: install a plugin' | \
  jq -r '.id + "\n\n" + .content + "\n--------\n"'
</code></pre>
<p>The output from that starts like so:</p>
<pre><code>llm/cli.py:1776

@cli.command(name="plugins")
@click.option("--all", help="Include built-in default plugins", is_flag=True)
def plugins_list(all):
    "List installed plugins"
    click.echo(json.dumps(get_plugins(all), indent=2))
--------

llm/cli.py:1791

@cli.command()
@click.argument("packages", nargs=-1, required=False)
@click.option(
    "-U", "--upgrade", is_flag=True, help="Upgrade packages to latest version"
)
...
def install(packages, upgrade, editable, force_reinstall, no_cache_dir):
    """Install packages from PyPI into the same environment as LLM"""
</code></pre>
<p>Getting this output was quite inconvenient, so I've <a href="https://github.com/simonw/llm/issues/853">opened an issue</a>.


    <p>Tags: <a href="https://simonwillison.net/tags/nomic">nomic</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/embeddings">embeddings</a>, <a href="https://simonwillison.net/tags/jq">jq</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making higher education more accessible to students in Pakistan]]></title>
        <id>https://news.mit.edu/2025/edufi-makes-higher-education-more-accessible-students-pakistan-0327</id>
        <link href="https://news.mit.edu/2025/edufi-makes-higher-education-more-accessible-students-pakistan-0327"/>
        <updated>2025-03-27T18:30:00.000Z</updated>
        <summary type="html"><![CDATA[EduFi, founded by an MIT alumna, provides low-interest student loans to families in Pakistan so more can attend college.]]></summary>
        <author>
            <name>Zach Winn | MIT News</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating CI with AWS CodeBuild: Parallel test execution now available]]></title>
        <id>e1f687c9f8d0323096db2a5d5faa8c59b4b6eab3</id>
        <link href="https://aws.amazon.com/blogs/aws/accelerating-ci-with-aws-codebuild-parallel-test-execution-now-available/"/>
        <updated>2025-03-27T18:14:29.000Z</updated>
        <summary type="html"><![CDATA[Speed up build times on CodeBuild with test splitting across multiple parallel build environments. Read how test splitting with CodeBuild works and how to get started.]]></summary>
        <author>
            <name>Sébastien Stormacq</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Announcing the AWS CDK L2 Construct for Amazon Cognito Identity Pools]]></title>
        <id>ac085208afc2314a676a8cf5155f0743a90de649</id>
        <link href="https://aws.amazon.com/blogs/devops/announcing-the-aws-cdk-l2-construct-for-amazon-cognito-identity-pools/"/>
        <updated>2025-03-27T17:01:53.000Z</updated>
        <summary type="html"><![CDATA[Today we’re announcing the general availability (GA) of the new Amazon Cognito Identity Pool Level 2 (L2) construct in the AWS Cloud Development Kit (AWS CDK). This construct simplifies the creation and management of identity pools, permissions, and provider integrations by providing intent-based APIs that help users securely manage their authenticated and unauthenticated (guest) users […]]]></summary>
        <author>
            <name>Adam Keller</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How engineers can use one-on-ones with their manager to accelerate career growth]]></title>
        <id>https://github.blog/developer-skills/career-growth/how-engineers-can-use-one-on-ones-with-their-manager-to-accelerate-career-growth/</id>
        <link href="https://github.blog/developer-skills/career-growth/how-engineers-can-use-one-on-ones-with-their-manager-to-accelerate-career-growth/"/>
        <updated>2025-03-27T15:31:22.000Z</updated>
        <summary type="html"><![CDATA[<p>Go beyond status updates and use these meetings to surface challenges, solve problems, and drive impact. </p>
<p>The post <a href="https://github.blog/developer-skills/career-growth/how-engineers-can-use-one-on-ones-with-their-manager-to-accelerate-career-growth/">How engineers can use one-on-ones with their manager to accelerate career growth</a> appeared first on <a href="https://github.blog">The GitHub Blog</a>.</p>]]></summary>
        <author>
            <name>The latest from GitHub's engineering team - The GitHub Blog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Daily Reading List – March 26, 2025 (#520)]]></title>
        <id>http://seroter.com/?p=21757</id>
        <link href="https://seroter.com/2025/03/26/daily-reading-list-march-26-2025-520/"/>
        <updated>2025-03-27T00:31:38.000Z</updated>
        <summary type="html"><![CDATA[Today's links look at observability 3.0, how vibe coding affects engineering managers, and how to build career resilience in uncertain times.]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three chapters at Cloudflare: Programmer to CTO to Board of Directors]]></title>
        <id>4KZNfa2772az1gUrIw1zpu</id>
        <link href="https://blog.cloudflare.com/three-chapters-at-cloudflare-programmer-to-cto-to-board-of-directors/"/>
        <updated>2025-03-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today, after more than 13 years at the company, I am joining Cloudflare’s board of directors and retiring from my full-time position as CTO.]]></summary>
        <author>
            <name>John Graham-Cumming</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function calling with Gemma]]></title>
        <id>https://simonwillison.net/2025/Mar/26/function-calling-with-gemma/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/26/function-calling-with-gemma/#atom-everything"/>
        <updated>2025-03-26T20:23:06.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://ai.google.dev/gemma/docs/capabilities/function-calling">Function calling with Gemma</a></strong></p>
Google's Gemma 3 model (the 27B variant is particularly capable, I've been trying it out <a href="https://ollama.com/library/gemma3">via Ollama</a>) supports function calling exclusively through prompt engineering. The official documentation describes two recommended prompts - both of them suggest that the tool definitions are passed in as JSON schema, but the way the model should request tool executions differs.</p>
<p>The first prompt uses Python-style function calling syntax:</p>
<blockquote>
<p><code>You have access to functions. If you decide to invoke any of the function(s),
 you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]</code></p>
<p><code>You SHOULD NOT include any other text in the response if you call a function</code></p>
</blockquote>
<p>(Always love seeing CAPITALS for emphasis in prompts, makes me wonder if they proved to themselves that capitalization makes a difference in this case.)</p>
<p>The second variant uses JSON instead:</p>
<blockquote>
<p><code>You have access to functions. If you decide to invoke any of the function(s),
you MUST put it in the format of {"name": function name, "parameters": dictionary of argument name and its value}</code></p>
<p><code>You SHOULD NOT include any other text in the response if you call a function</code></p>
</blockquote>
<p>This is a neat illustration of the fact that all of these fancy tool using LLMs are still using effectively the same pattern as was described in <a href="https://react-lm.github.io/">the ReAct paper</a> back in November 2022. Here's <a href="https://til.simonwillison.net/llms/python-react-pattern">my implementation of that pattern</a> from March 2023.

    <p><small></small>Via <a href="https://news.ycombinator.com/item?id=43451406">Hacker News</a></small></p>


    <p>Tags: <a href="https://simonwillison.net/tags/prompt-engineering">prompt-engineering</a>, <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/gemma">gemma</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Firewall support for AWS Amplify hosted sites]]></title>
        <id>d7aaff9fd6cc3ace262f1111bd106ce6f5fcbf35</id>
        <link href="https://aws.amazon.com/blogs/aws/firewall-support-for-aws-amplify-hosted-sites/"/>
        <updated>2025-03-26T20:05:15.000Z</updated>
        <summary type="html"><![CDATA[AWS Amplify Hosting now offers direct integration with AWS WAF, allowing users to apply web application firewall protection to their hosted apps through the console or infrastructure as code, eliminating the need for complex Amazon CloudFront configurations.]]></summary>
        <author>
            <name>Sébastien Stormacq</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quoting @OpenAIDevs]]></title>
        <id>https://simonwillison.net/2025/Mar/26/openaidevs/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/26/openaidevs/#atom-everything"/>
        <updated>2025-03-26T19:27:38.000Z</updated>
        <summary type="html"><![CDATA[<blockquote cite="https://twitter.com/OpenAIDevs/status/1904957755829481737"><p>MCP 🤝 OpenAI Agents SDK</p>
<p>You can now connect your Model Context Protocol servers to Agents: <a href="https://openai.github.io/openai-agents-python/mcp/">openai.github.io/openai-agents-python/mcp/</a></p>
<p>We’re also working on MCP support for the OpenAI API and ChatGPT desktop app—we’ll share some more news in the coming months.</p></blockquote>
<p class="cite">&mdash; <a href="https://twitter.com/OpenAIDevs/status/1904957755829481737">@OpenAIDevs</a></p>

    <p>Tags: <a href="https://simonwillison.net/tags/model-context-protocol">model-context-protocol</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai-agents">ai-agents</a>, <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From virtual machine to Kubernetes to serverless: How dacadoo saved 78% on cloud costs and automated operations]]></title>
        <id>bb3f454cb2a9eac3e8c23f5da1e2dd14ec278f6e</id>
        <link href="https://aws.amazon.com/blogs/architecture/from-virtual-machine-to-kubernetes-to-serverless-how-dacadoo-saved-78-on-cloud-costs-and-automated-operations/"/>
        <updated>2025-03-26T14:59:52.000Z</updated>
        <summary type="html"><![CDATA[In this post, we walk you step-by-step through dacadoo's journey of embracing managed services, highlighting their architectural decisions as we go.]]></summary>
        <author>
            <name>Andreas Gehrig</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIT Maritime Consortium sets sail]]></title>
        <id>https://news.mit.edu/2025/mit-maritime-consortium-sets-sail-0326</id>
        <link href="https://news.mit.edu/2025/mit-maritime-consortium-sets-sail-0326"/>
        <updated>2025-03-26T12:55:00.000Z</updated>
        <summary type="html"><![CDATA[A new international collaboration unites MIT and maritime industry leaders to develop nuclear propulsion technologies, alternative fuels, data-powered strategies for operation, and more.]]></summary>
        <author>
            <name>Anne Wilson | Department of Mechanical Engineering</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Note on 26th March 2025]]></title>
        <id>https://simonwillison.net/2025/Mar/26/notes/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/26/notes/#atom-everything"/>
        <updated>2025-03-26T06:11:30.000Z</updated>
        <summary type="html"><![CDATA[<p>I've added a new content type to my blog: <strong>notes</strong>. These join my existing types: <a href="https://simonwillison.net/search/?type=entry">entries</a>, <a href="https://simonwillison.net/search/?type=blogmark">bookmarks</a> and <a href="https://simonwillison.net/search/?type=quotation">quotations</a>.</p>
<p>A note is a little bit like a bookmark without a link. They're for short form writing - thoughts or images that don't warrant a full entry with a title. The kind of things I used to post to Twitter, but that don't feel right to cross-post to multiple social networks (Mastodon and Bluesky, for example.)</p>
<p>I was partly inspired by Molly White's <a href="https://www.mollywhite.net/micro">short thoughts, notes, links, and musings</a>.</p>
<p>I've been thinking about this for a while, but the amount of work involved in modifying all of the parts of my site that handle the three different content types was daunting. Then this evening I tried running my blog's source code (using <a href="https://github.com/simonw/files-to-prompt">files-to-prompt</a> and <a href="https://llm.datasette.io/">LLM</a>) through <a href="https://simonwillison.net/2025/Mar/25/gemini/">the new Gemini 2.5 Pro</a>:</p>
<div class="highlight highlight-source-shell"><pre>files-to-prompt <span class="pl-c1">.</span> -e py -c <span class="pl-k">|</span> \
  llm -m gemini-2.5-pro-exp-03-25 -s \
  <span class="pl-s"><span class="pl-pds">'</span>I want to add a new type of content called a Note,</span>
<span class="pl-s">  similar to quotation and bookmark and entry but it</span>
<span class="pl-s">  only has a markdown text body. Output all of the</span>
<span class="pl-s">  code I need to add for that feature and tell me</span>
<span class="pl-s">  which files to add  the code to.<span class="pl-pds">'</span></span></pre></div>

<p>Gemini gave me <a href="https://gist.github.com/simonw/6d9fb3e33e7105d391a31367d6a235de#response">a detailed 13 step plan</a> covering all of the tedious changes I'd been avoiding having to figure out!</p>
<p>The code <a href="https://github.com/simonw/simonwillisonblog/pull/527">is in this PR</a>, which touched 18 different files. The whole project took around 45 minutes start to finish.</p>
<p>(I used Claude to <a href="https://claude.ai/share/17656d59-6f52-471e-8aeb-6abbe1464471">brainstorm names</a> for the feature - I had it come up with possible nouns and then "rank those by least pretentious to most pretentious", and "notes" came out on top.)</p>
<p>This is now far too long for a note and should really be upgraded to an entry, but I need to post a first note to make sure everything is working as it should.</p>

    <p>Tags: <a href="https://simonwillison.net/tags/blogging">blogging</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/molly-white">molly-white</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Project Jengo for Sable — final winners!]]></title>
        <id>7vpLPW7ezoVN0nUbSRGmeJ</id>
        <link href="https://blog.cloudflare.com/project-jengo-for-sable-final-winners/"/>
        <updated>2025-03-26T05:00:00.000Z</updated>
        <summary type="html"><![CDATA[With Cloudflare’s victory against patent trolls Sable IP and Sable Networks in the books, it’s time to close out the case’s Project Jengo competition.]]></summary>
        <author>
            <name>Emily Terrell</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quoting Manuel Hoffmann, Frank Nagle, Yanuo Zhou]]></title>
        <id>https://simonwillison.net/2025/Mar/26/value-of-open-source/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/26/value-of-open-source/#atom-everything"/>
        <updated>2025-03-26T03:49:18.000Z</updated>
        <summary type="html"><![CDATA[<blockquote cite="https://www.hbs.edu/faculty/Pages/item.aspx?num=65230"><p>We estimate the supply-side value of widely-used OSS is $4.15 billion, but that the demand-side value is much larger at $8.8 trillion. We find that firms would need to spend 3.5 times more on software than they currently do if OSS did not exist.</p></blockquote>
<p class="cite">&mdash; <a href="https://www.hbs.edu/faculty/Pages/item.aspx?num=65230">Manuel Hoffmann, Frank Nagle, Yanuo Zhou</a>, The Value of Open Source Software, Harvard Business School</p>

    <p>Tags: <a href="https://simonwillison.net/tags/open-source">open-source</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Daily Reading List – March 25, 2025 (#519)]]></title>
        <id>http://seroter.com/?p=21735</id>
        <link href="https://seroter.com/2025/03/25/daily-reading-list-march-25-2025-519/"/>
        <updated>2025-03-26T00:16:13.000Z</updated>
        <summary type="html"><![CDATA[Today's links look at how to use LLMs to write code, what happens when you oversell your abilities, and 20 business terms that techies should know.]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context is all you need: Better AI results with custom instructions]]></title>
        <id>https://code.visualstudio.com/blogs/2025/03/26/custom-instructions</id>
        <link href="https://code.visualstudio.com/blogs/2025/03/26/custom-instructions"/>
        <updated>2025-03-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Announcing the general availability of custom instructions for VS Code.
Read the full article]]></summary>
        <author>
            <name>Visual Studio Code - Code Editing. Redefined.</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing 4o Image Generation]]></title>
        <id>https://simonwillison.net/2025/Mar/25/introducing-4o-image-generation/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/25/introducing-4o-image-generation/#atom-everything"/>
        <updated>2025-03-25T21:11:23.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://openai.com/index/introducing-4o-image-generation/">Introducing 4o Image Generation</a></strong></p>
When OpenAI first announced GPT-4o <a href="https://simonwillison.net/2024/May/13/gpt-4o/">back in May 2024</a> one of the most exciting features was true multi-modality in that it could both input <em>and</em>  output audio and images. The "o" stood for "omni", and the image output examples <a href="https://openai.com/index/hello-gpt-4o/">in that launch post</a> looked really impressive.</p>
<p>It's taken them over ten months (and Gemini <a href="https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation/">beat them to it</a>) but today they're finally making those image generation abilities available, live right now in ChatGPT for paying customers.</p>
<p>My test prompt for any model that can manipulate incoming images is "Turn this into a selfie with a bear", because you should never take a selfie with a bear! I fed ChatGPT <a href="https://static.simonwillison.net/static/2025/selfie.jpg">this selfie</a> and got back this result:</p>
<p><img alt="It's a selfie, there's a grizzly bear over my shoulder smiling." src="https://static.simonwillison.net/static/2025/selfie-with-a-bear.jpg" /></p>
<p>That's pretty great! It mangled the text on my T-Shirt (which says "LAWRENCE.COM" in a creative font) and added a second visible AirPod. It's very clearly me though, and that's definitely a bear.</p>
<p>There are plenty more examples in <a href="https://openai.com/index/introducing-4o-image-generation/">OpenAI's launch post</a>, but as usual the most interesting details are tucked away in <a href="https://openai.com/index/gpt-4o-image-generation-system-card-addendum/">the updates to the system card</a>. There's lots in there about their approach to safety and bias, including a section on "Ahistorical and Unrealistic Bias" which feels inspired by Gemini's <a href="https://blog.google/products/gemini/gemini-image-generation-issue/">embarrassing early missteps</a>.</p>
<p>One section that stood out to me is their approach to images of public figures. The new policy is much more permissive than for DALL-E - highlights mine:</p>
<blockquote>
<p>4o image generation is capable, in many instances, of generating a depiction of a public figure based solely on a text prompt.</p>
<p><strong>At launch, we are not blocking the capability to generate adult public figures</strong> but are instead implementing the same safeguards that we have implemented for editing images of photorealistic uploads of people. For instance, this includes seeking to block the generation of photorealistic images of public figures who are minors and of material that violates our policies related to violence, hateful imagery, instructions for illicit activities, erotic content, and other areas. <strong>Public figures who wish for their depiction not to be generated can opt out</strong>.</p>
<p>This approach is more fine-grained than the way we dealt with public figures in our DALL·E series of models, where we used technical mitigations intended to prevent any images of a public figure from being generated. <strong>This change opens the possibility of helpful and beneficial uses in areas like educational, historical, satirical and political speech</strong>. After launch, we will continue to monitor usage of this capability, evaluating our policies, and will adjust them if needed.</p>
</blockquote>
<p>Given that "public figures who wish for their depiction not to be generated can opt out" I wonder if we'll see a stampede of public figures to do exactly that!</p>
<p><strong>Update</strong>: There's significant confusion right now over this new feature because it is being rolled out gradually but older ChatGPT can still generate images using DALL-E instead... and there is no visual indication in the ChatGPT UI explaining which image generation method it used!</p>
<p>OpenAI made the same mistake last year <a href="https://simonwillison.net/2024/May/15/chatgpt-in-4o-mode/">when they announced ChatGPT advanced voice mode</a> but failed to clarify that ChatGPT was still running the previous, less impressive voice implementation.</p>
<p><strong>Update 2</strong>: Images created with DALL-E through the ChatGPT web interface now show a note with a warning:</p>
<p><img alt="Created with DALL-E with an information icon. Clicking it reveals DALL-E is OpenAI's legacy image generation model. A new model is rolling out in ChatGPT soon." src="https://static.simonwillison.net/static/2025/dall-e-warning.jpg" />


    <p>Tags: <a href="https://simonwillison.net/tags/openai">openai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/multi-modal-output">multi-modal-output</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/ai-ethics">ai-ethics</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/chatgpt">chatgpt</a>, <a href="https://simonwillison.net/tags/dalle">dalle</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effortless application modernization: migrate to Amazon EKS with existing NLB setup]]></title>
        <id>5690098eb267e3dff09e60d62f4d50b7167d25a3</id>
        <link href="https://aws.amazon.com/blogs/containers/effortless-application-modernization-migrate-to-amazon-eks-with-existing-nlb-setup/"/>
        <updated>2025-03-25T20:56:13.000Z</updated>
        <summary type="html"><![CDATA[This post was co-authored by Henrique Santana, Container Specialist, AWS and Luis Felipe, Principal Solutions Architect, AWS. Introduction Many organizations have built their infrastructure using Amazon Elastic Compute Cloud (Amazon EC2) and Network Load Balancer (NLB), often with security policies built around the NLB’s static IP addresses. As these organizations adopt containerization and move to […]]]></summary>
        <author>
            <name>pmm</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Putting Gemini 2.5 Pro through its paces]]></title>
        <id>https://simonwillison.net/2025/Mar/25/gemini/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/25/gemini/#atom-everything"/>
        <updated>2025-03-25T20:45:58.000Z</updated>
        <summary type="html"><![CDATA[<p>There's a new release from Google Gemini this morning: <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking">the first in the Gemini 2.5</a> series. Google call it "a thinking model, designed to tackle increasingly complex problems". It's already sat at the top of the <a href="https://lmarena.ai/?leaderboard">LM Arena leaderboard</a>, and from initial impressions looks like it may deserve that top spot.</p>
<p>I just released <a href="https://github.com/simonw/llm-gemini/releases/tag/0.16">llm-gemini 0.16</a> adding support for the new model to my <a href="https://llm.datasette.io/">LLM</a> command-line tool. Let's try it out.</p>

<ul>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#the-pelican-riding-a-bicycle">The pelican riding a bicycle</a></li>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#transcribing-audio">Transcribing audio</a></li>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#bounding-boxes">Bounding boxes</a></li>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#characteristics">More characteristics of the model</a></li>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#gemini-2-5-pro-is-a-very-strong-new-model">Gemini 2.5 Pro is a very strong new model</a></li>
  <li><a href="https://simonwillison.net/2025/Mar/25/gemini/#update-it-s-very-good-at-code">Update: it's very good at code</a></li>
</ul>

<h4 id="the-pelican-riding-a-bicycle">The pelican riding a bicycle</h4>
<p>First up, my classic <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">generate an SVG of a pelican riding a bicycle</a> prompt.</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span> Upgrade the plugin</span>
llm install -U llm-gemini

<span class="pl-c"><span class="pl-c">#</span> Now run the prompt:</span>
llm -m gemini-2.5-pro-exp-03-25 <span class="pl-s"><span class="pl-pds">"</span>Generate an SVG of a pelican riding a bicycle<span class="pl-pds">"</span></span></pre></div>
<p>It's pretty solid!</p>
<p><img src="https://static.simonwillison.net/static/2025/gemini-2.5-pro-pelican.jpg" alt="The pelican is perched at an angle which suggests an effort at having it make contact with the pedals. It has a good large beak." style="max-width: 100%;" /></p>
<p>Here's <a href="https://gist.github.com/simonw/c34f7f0c94afcbeab77e170511f6f51f">the full transcript</a>.</p>
<p>This task is meant to be almost impossible: pelicans are the wrong shape to ride bicycles! Given that, I think this is a good attempt - I like it slightly better than my previous favourite <a href="https://simonwillison.net/2025/Feb/24/claude-37-sonnet-and-claude-code/">Claude 3.7 Sonnet</a>, which produced this a month ago:</p>
<p><img src="https://static.simonwillison.net/static/2025/pelican-claude-3.7-sonnet.svg" alt="The bicycle has pokes but the pedal is in the wrong place, and the pelican perches uncomfortably on the seat with weird looking wings." style="max-width: 100%;" /></p>
<h4 id="transcribing-audio">Transcribing audio</h4>
<p>I had <a href="https://static.simonwillison.net/static/2025/russian-pelican-in-spanish.mp3">an MP3</a> lying around from <a href="https://simonwillison.net/2024/Oct/26/russian-spanish-pelican/">a previous experiment</a> which mixes English and Spanish. I tried running it with the prompt <code>transcribe</code> to see what would happen:</p>
<div class="highlight highlight-source-shell"><pre>llm -m gemini-2.5-pro-exp-03-25 <span class="pl-s"><span class="pl-pds">'</span>transcribe<span class="pl-pds">'</span></span> \
  -a https://static.simonwillison.net/static/2025/russian-pelican-in-spanish.mp3</pre></div>
<p>I got back this, with timestamps interspersed with the text:</p>
<blockquote>
<p>I need you [ 0m0s450ms ] to pretend [ 0m0s880ms ] to be [ 0m0s990ms ] a California [ 0m1s560ms ] brown [ 0m1s850ms ] pelican [ 0m2s320ms ] with [ 0m2s480ms ] a very [ 0m2s990ms ] thick [ 0m3s290ms ] Russian [ 0m3s710ms ] accent, [ 0m4s110ms ] but [ 0m4s540ms ] you [ 0m4s640ms ] talk [ 0m4s830ms ] to me [ 0m4s960ms ] exclusively [ 0m5s660ms ] in Spanish. [ 0m6s200ms ] Oye, [ 0m8s930ms ] camarada, [ 0m9s570ms ] aquí [ 0m10s240ms ] está [ 0m10s590ms ] tu [ 0m10s740ms ] pelícano [ 0m11s370ms ] californiano [ 0m12s320ms ] con [ 0m12s520ms ] acento [ 0m13s100ms ] ruso. [ 0m13s540ms ] Qué [ 0m14s230ms ] tal, [ 0m14s570ms ] tovarisch? [ 0m15s210ms ] Listo [ 0m15s960ms ] para [ 0m16s190ms ] charlar [ 0m16s640ms ] en [ 0m16s750ms ] español? [ 0m17s250ms ] How's [ 0m19s834ms ] your [ 0m19s944ms ] day [ 0m20s134ms ] today? [ 0m20s414ms ] Mi [ 0m22s654ms ] día [ 0m22s934ms ] ha [ 0m23s4ms ] sido [ 0m23s464ms ] volando [ 0m24s204ms ] sobre [ 0m24s594ms ] las [ 0m24s844ms ] olas, [ 0m25s334ms ] buscando [ 0m26s264ms ] peces [ 0m26s954ms ] y [ 0m27s84ms ] disfrutando [ 0m28s14ms ] del [ 0m28s244ms ] sol [ 0m28s664ms ] californiano. [ 0m29s444ms ] Y [ 0m30s314ms ] tú, [ 0m30s614ms ] amigo, ¿ [ 0m31s354ms ] cómo [ 0m31s634ms ] ha [ 0m31s664ms ] estado [ 0m31s984ms ] tu [ 0m32s134ms ] día? [ 0m32s424ms ]</p>
</blockquote>
<p>This inspired me to try again, this time including <a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">a JSON schema</a> (using LLM's custom schema DSL):</p>
<div class="highlight highlight-source-shell"><pre>llm -m gemini-2.5-pro-exp-03-25 <span class="pl-s"><span class="pl-pds">'</span>transcribe<span class="pl-pds">'</span></span> \
  -a https://static.simonwillison.net/static/2025/russian-pelican-in-spanish.mp3 \
  --schema-multi <span class="pl-s"><span class="pl-pds">'</span>timestamp str: mm:ss,text, language: two letter code<span class="pl-pds">'</span></span></pre></div>
<p>I got an <em>excellent</em> response from that:</p>
<div class="highlight highlight-source-json"><pre>{
  <span class="pl-ent">"items"</span>: [
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>en<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:00<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>es<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>Oye, camarada. Aquí está tu pelícano californiano con acento ruso.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:08<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>es<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>¿Qué tal, Tovarish? ¿Listo para charlar en español?<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:13<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>en<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>How's your day today?<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:19<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>es<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>Mi día ha sido volando sobre las olas, buscando peces y disfrutando del sol californiano.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:22<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"language"</span>: <span class="pl-s"><span class="pl-pds">"</span>es<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>¿Y tú, amigo, cómo ha estado tu día?<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:30<span class="pl-pds">"</span></span>
    }
  ]
}</pre></div>
<p>I confirmed that the timestamps match the audio. This is fantastic.</p>
<p>Let's try that against a ten minute snippet of <a href="https://realpython.com/podcasts/rpp/236/">a podcast episode</a> I was on:</p>
<div class="highlight highlight-source-shell"><pre>llm -m gemini-2.5-pro-exp-03-25 \
  <span class="pl-s"><span class="pl-pds">'</span>transcribe, first speaker is Christopher, second is Simon<span class="pl-pds">'</span></span> \
  -a ten-minutes-of-podcast.mp3 \
  --schema-multi <span class="pl-s"><span class="pl-pds">'</span>timestamp str: mm:ss, text, speaker_name<span class="pl-pds">'</span></span></pre></div>
<p>Useful LLM trick: you can use <code>llm logs -c --data</code> to get just the JSON data from the most recent prompt response, so I ran this:</p>
<div class="highlight highlight-source-shell"><pre>llm logs -c --data <span class="pl-k">|</span> jq</pre></div>
<p>Here's <a href="https://gist.github.com/simonw/23d4528e23e4897e0c43458e7c46c360">the full output JSON</a>, which starts and ends like this:</p>
<div class="highlight highlight-source-json"><pre>{
  <span class="pl-ent">"items"</span>: [
    {
      <span class="pl-ent">"speaker_name"</span>: <span class="pl-s"><span class="pl-pds">"</span>Christopher<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>on its own and and it has this sort of like a it's like a you know old tree in the forest, you know, kind of thing that you've built, so.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:00<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"speaker_name"</span>: <span class="pl-s"><span class="pl-pds">"</span>Simon<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>There's also like I feel like with online writing, never ever like stick something online just expect people to find it. You have to So one of the great things about having a blog is I can be in a conversation about something and somebody ask a question, I can say, oh, I wrote about that two and a half years ago and give people a link.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>00:06<span class="pl-pds">"</span></span>
    },

    {
      <span class="pl-ent">"speaker_name"</span>: <span class="pl-s"><span class="pl-pds">"</span>Simon<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>So on that basis, Chat and I can't remember if the free version of Chat GPT has code interpreter.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>09:45<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"speaker_name"</span>: <span class="pl-s"><span class="pl-pds">"</span>Simon<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>I hope I think it does.<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>09:50<span class="pl-pds">"</span></span>
    },
    {
      <span class="pl-ent">"speaker_name"</span>: <span class="pl-s"><span class="pl-pds">"</span>Christopher<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"text"</span>: <span class="pl-s"><span class="pl-pds">"</span>Okay. So this is like the basic paid one, maybe the $20 month because I know there's like a $200 one that's a little steep for like a basic<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"timestamp"</span>: <span class="pl-s"><span class="pl-pds">"</span>09:51<span class="pl-pds">"</span></span>
    }
  ]
}</pre></div>
<p>A spot check of the timestamps showed them in the right place. Gemini 2.5 supports long context prompts so it's possible this works well for much longer audio files - it would be interesting to dig deeper and try that out.</p>
<h4 id="bounding-boxes">Bounding boxes</h4>
<p>One of my favourite features of previous Gemini models is their support for bounding boxes: you can prompt them to return boxes around objects in images.</p>
<p>I built a separate tool for experimenting with this feature in August last year, which I described in <a href="https://simonwillison.net/2024/Aug/26/gemini-bounding-box-visualization/">Building a tool showing how Gemini Pro can return bounding boxes for objects in images</a>. I've now upgraded that tool to add support the new model.</p>
<p>You can access it at <a href="https://tools.simonwillison.net/gemini-bbox">tools.simonwillison.net/gemini-bbox</a> - you'll need to provide your own Gemini API key which is sent directly to their API from your browser (it won't be logged by an intermediary).</p>
<p>I tried it out on <a href="https://static.simonwillison.net/static/2025/challenging-pelicans.jpg">a challenging photograph</a> of some pelicans... and it worked extremely well:</p>
<p>My prompt was:</p>
<blockquote>
<p><code>Return bounding boxes around pelicans as JSON arrays [ymin, xmin, ymax, xmax]</code></p>
</blockquote>
<p>The Gemini models are all trained to return bounding boxes scaled between 0 and 100. My tool knows how to convert those back to the same dimensions as the input image.</p>
<p>Here's what the visualized result looked like:</p>
<p><img src="https://static.simonwillison.net/static/2025/gemini-pelicans-bbox.jpg" alt="Pelicans with bounding boxes around them" style="max-width: 100%;" /></p>
<p>It got almost all of them! I like how it didn't draw a box around the one egret that had made it into the photo.</p>
<h4 id="characteristics">More characteristics of the model</h4>

<p>Here's the <a href="https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-exp-03-25">official model listing</a> in the Gemini docs. Key details:</p>
<ul>
  <li>Input token limit: 1,000,000</li>
  <li>Output token limit: 64,000 - this is a <em>huge</em> upgrade, all of the other listed models have 8,192 for this (correction: <a href="https://deepmind.google/technologies/gemini/flash-thinking/">Gemini 2.0 Flash Thinking</a> also had a 64,000 output length)</li>
  <li>Knowledge cut-off: January 2025 - an improvement on Gemini 2.0's August 2024</li>
</ul>

<h4 id="gemini-2-5-pro-is-a-very-strong-new-model">Gemini 2.5 Pro is a very strong new model</h4>
<p>I've hardly scratched the surface when it comes to trying out Gemini 2.5 Pro so far. How's its creative writing? Factual knowledge about the world? Can it write great code in Python, JavaScript, Rust and more?</p>
<p>The Gemini family of models have capabilities that set them apart from other models:</p>
<ul>
<li>Long context length - Gemini 2.5 Pro supports up to 1 million tokens</li>
<li>Audio input - something which few other models support, certainly not at this length and with this level of timestamp accuracy</li>
<li>Accurate bounding box detection for image inputs</li>
</ul>
<p>My experiments so far with these capabilities indicate that Gemini 2.5 Pro really is a very strong new model. I'm looking forward to exploring more of what it can do.</p>

<h4 id="update-it-s-very-good-at-code">Update: it's very good at code</h4>
<p>I spent this evening trying it out for coding tasks, and it's very, very impressive. I'm seeing results for Python that feel comparable to my previous favourite Claude 3.7 Sonnet, and appear to be benefitting from Gemini 2.5 Pro's default reasoning mode and long context.</p>
<p>I've been wanting to add a new content type of "notes" to my blog for quite a while now, but I was put off by the anticipated tedium of all of the different places in the codebase that would need to be updated.</p>
<p>That feature is now live. <a href="https://simonwillison.net/2025/Mar/26/notes/">Here are my notes</a> on creating that notes feature using Gemini 2.5 Pro. It crunched through my entire codebase and figured out all of the places I needed to change - 18 files in total, as you can see in <a href="https://github.com/simonw/simonwillisonblog/pull/527/files">the resulting PR</a>. The whole project took about 45 minutes from start to finish - averaging less than three minutes per file I had to modify.</p>
<p>I've thrown a whole bunch of other coding challenges at it, and the bottleneck on evaluating them has become my own mental capacity to review the resulting code!</p>
<p>Here's another, more complex example. This hasn't resulted in actual running code yet but it took a big bite out of an architectural design problem I've been stewing on for a very long time.</p>

<p>My <a href="https://llm.datasette.io/">LLM project</a> needs support for tools - a way to teach different LLMs how to request tool execution, then have those tools (implemented in Python) run and return their results back to the models.</p>

<p>Designing this is really hard, because I need to create an abstraction that works across multiple different model providers, each powered by a different plugin.</p>

<p>Could Gemini 2.5 Pro help unblock me by proposing an architectural approach that might work?</p>

<p>I started by combining the Python and Markdown files for my <a href="https://github.com/simonw/sqlite-utils">sqlite-utils</a>, <a href="https://github.com/simonw/llm">llm</a>, <a href="https://github.com/simonw/llm-gemini">llm-gemini</a>, and <a href="https://github.com/simonw/llm-anthropic">llm-anthropic</a> repositories into one big document:</p>
<div class="highlight highlight-source-shell"><pre>files-to-prompt sqlite-utils llm llm-gemini llm-anthropic -e md -e py -c</pre></div>
<p>Piping it through <a href="https://github.com/simonw/ttok">ttok</a> showed that to be 316,098 tokens (using the OpenAI tokenizer, but the Gemini tokenizer is likely a similar number).</p>
<p>Then I fed that all into Gemini 2.5 Pro with the following prompt:</p>
<blockquote>
<p>Provide a detailed design proposal for adding tool calling support to LLM</p>
<p>Different model plugins will have different ways of executing tools, but LLM itself should provide both a Python and a CLI abstraction over these differences.</p>
<p>Tool calling involves passing tool definitions to a model, the model then replies with tools it would like executed, the harness code then executes those and passes the result back to the model and continues in a loop until the model stops requesting tools. This should happen as part of the existing llm.Conversation mechanism.</p>
<p>At the Python layer tools should be provided as Python functions that use type hints, which can then be converted into JSON schema using Pydantic - similar to how the existing schema= mechanism works.</p>
<p>For the CLI option tools will be provided as an option passed to the command, similar to how the sqlite-utils convert CLI mechanism works.'</p>
</blockquote>
<p>I've been thinking about this problem for over a year now. <a href="https://gist.github.com/simonw/481f46a779f20d8650b4fc5854f070c5">Gemini 2.5 Pro's response</a> isn't exactly what I'm going to do, but it did include a flurry of genuinely useful suggestions to help me craft my final approach.</p>
<p>I hadn't thought about the need for asyncio support for tool functions at all, so this idea from Gemini 2.5 Pro was very welcome:</p>
<blockquote>
<p>The <code>AsyncConversation.prompt()</code> method will mirror the synchronous API, accepting the <code>tools</code> parameter and handling the tool calling loop using <code>async/await</code> for tool execution if the tool functions themselves are async. If tool functions are synchronous, they will be run in a thread pool executor via <code>asyncio.to_thread</code>.</p>
</blockquote>
<p>Watching Gemini crunch through some of the more complex problems I've been dragging my heels on reminded me of something Harper Reed wrote <a href="https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/">about his   workflow with LLMs for code</a>:</p>
<blockquote>
<p>My hack to-do list is empty because I built everything. I keep thinking of new things and knocking them out while watching a movie or something.</p>
</blockquote>
<p>I've been stressing quite a bit about my backlog of incomplete projects recently. I don't think Gemini 2.5 Pro is quite going to work through all of them while I'm watching TV, but it does feel like it's going to help make a significant dent in them.</p>
    
        <p>Tags: <a href="https://simonwillison.net/tags/google">google</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/gemini">gemini</a>, <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detailed geographic information for all AWS Regions and Availability Zones is now available]]></title>
        <id>8ef572ee74a40fa66e5321cfb5d53c61268fb8c7</id>
        <link href="https://aws.amazon.com/blogs/aws/now-available-geography-information-for-all-aws-regions-and-availability-zones/"/>
        <updated>2025-03-25T19:23:46.000Z</updated>
        <summary type="html"><![CDATA[AWS is expanding its global infrastructure, now providing increased transparency about the specific geographic locations of its 114 Availability Zones across 36 regions to help customers meet regulatory requirements and make informed deployment decisions.]]></summary>
        <author>
            <name>Prasad Rao</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Build and deploy Remote Model Context Protocol (MCP) servers to Cloudflare]]></title>
        <id>4e3J8mxEIN24iNKfw9ToEH</id>
        <link href="https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/"/>
        <updated>2025-03-25T13:59:00.000Z</updated>
        <summary type="html"><![CDATA[You can now build and deploy remote MCP servers to Cloudflare, and we handle the hard parts of building remote MCP servers for you.]]></summary>
        <author>
            <name>Brendan Irvine-Broque</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open-sourcing OpenPubkey SSH (OPKSSH): integrating single sign-on with SSH]]></title>
        <id>01zA7RtUKkhrUeINJ9AIS3</id>
        <link href="https://blog.cloudflare.com/open-sourcing-openpubkey-ssh-opkssh-integrating-single-sign-on-with-ssh/"/>
        <updated>2025-03-25T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[OPKSSH (OpenPubkey SSH) is now open-sourced as part of the OpenPubkey project.]]></summary>
        <author>
            <name>Ethan Heilman</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quoting Greg Kamradt]]></title>
        <id>https://simonwillison.net/2025/Mar/25/greg-kamradt/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/25/greg-kamradt/#atom-everything"/>
        <updated>2025-03-25T06:18:08.000Z</updated>
        <summary type="html"><![CDATA[<blockquote cite="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025"><p>Today we’re excited to launch ARC-AGI-2 to challenge the new frontier. ARC-AGI-2 is even harder for AI (in particular, AI reasoning systems), while maintaining the same relative ease for humans. Pure LLMs score 0% on ARC-AGI-2, and public AI reasoning systems achieve only single-digit percentage scores. In contrast, every task in ARC-AGI-2 has been solved by at least 2 humans in under 2 attempts. [...]</p>
<p>All other AI benchmarks focus on superhuman capabilities or specialized knowledge by testing "PhD++" skills. ARC-AGI is the only benchmark that takes the opposite design choice – by focusing on tasks that are relatively easy for humans, yet hard, or impossible, for AI, we shine a spotlight on capability gaps that do not spontaneously emerge from "scaling up".</p></blockquote>
<p class="cite">&mdash; <a href="https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025">Greg Kamradt</a>, ARC-AGI-2</p>

    <p>Tags: <a href="https://simonwillison.net/tags/evals">evals</a>, <a href="https://simonwillison.net/tags/ai">ai</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[shot-scraper 1.8]]></title>
        <id>https://simonwillison.net/2025/Mar/25/shot-scraper/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/25/shot-scraper/#atom-everything"/>
        <updated>2025-03-25T01:59:38.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://github.com/simonw/shot-scraper/releases/tag/1.8">shot-scraper 1.8</a></strong></p>
I've added a new feature to <a href="https://shot-scraper.datasette.io/">shot-scraper</a> that makes it easier to share scripts for other people to use with the <a href="">shot-scraper javascript</a> command.</p>
<p><code>shot-scraper javascript</code> lets you load up a web page in an invisible Chrome browser (via Playwright), execute some JavaScript against that page and output the results to your terminal. It's a fun way of running complex screen-scraping routines as part of a terminal session, or even chained together with other commands using pipes.</p>
<p>The <code>-i/--input</code> option lets you load that JavaScript from a file on disk - but now you can also use a <code>gh:</code> prefix to specify loading code from GitHub instead.</p>
<p>To quote <a href="https://github.com/simonw/shot-scraper/releases/tag/1.8">the release notes</a>:</p>
<blockquote>
<p><code>shot-scraper javascript</code> can now optionally <a href="https://shot-scraper.datasette.io/en/stable/javascript.html#running-javascript-from-github">load scripts hosted on GitHub</a> via the new <code>gh:</code> prefix to the <code>shot-scraper javascript -i/--input</code> option. <a href="https://github.com/simonw/shot-scraper/issues/173">#173</a></p>
<p>Scripts can be referenced as <code>gh:username/repo/path/to/script.js</code> or, if the GitHub user has created a dedicated <code>shot-scraper-scripts</code> repository and placed scripts in the root of it, using <code>gh:username/name-of-script</code>.</p>
<p>For example, to run this <a href="https://github.com/simonw/shot-scraper-scripts/blob/main/readability.js">readability.js</a> script against any web page you can use the following:</p>
<pre><code>shot-scraper javascript --input gh:simonw/readability \
  https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/
</code></pre>
</blockquote>
<p>The <a href="https://gist.github.com/simonw/60e196ec39a5a75dcabfd75fbe911a4c">output from that example</a> starts like this:</p>
<div class="highlight highlight-source-json"><pre>{
    <span class="pl-ent">"title"</span>: <span class="pl-s"><span class="pl-pds">"</span>Qwen2.5-VL-32B: Smarter and Lighter<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"byline"</span>: <span class="pl-s"><span class="pl-pds">"</span>Simon Willison<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"dir"</span>: <span class="pl-c1">null</span>,
    <span class="pl-ent">"lang"</span>: <span class="pl-s"><span class="pl-pds">"</span>en-gb<span class="pl-pds">"</span></span>,
    <span class="pl-ent">"content"</span>: <span class="pl-s"><span class="pl-pds">"</span>&lt;div id=<span class="pl-cce">\"</span>readability-page-1<span class="pl-cce">\"...</span></pre></div>

<p>My <a href="https://github.com/simonw/shot-scraper-scripts">simonw/shot-scraper-scripts</a> repo only has that one file in it so far, but I'm looking forward to growing that collection and hopefully seeing other people create and share their own <code>shot-scraper-scripts</code> repos as well.</p>
<p>This feature is an imitation of <a href="https://github.com/simonw/llm/issues/809">a similar feature</a> that's coming in the next release of LLM.


    <p>Tags: <a href="https://simonwillison.net/tags/playwright">playwright</a>, <a href="https://simonwillison.net/tags/shot-scraper">shot-scraper</a>, <a href="https://simonwillison.net/tags/scraping">scraping</a>, <a href="https://simonwillison.net/tags/javascript">javascript</a>, <a href="https://simonwillison.net/tags/projects">projects</a>, <a href="https://simonwillison.net/tags/github">github</a>, <a href="https://simonwillison.net/tags/annotated-release-notes">annotated-release-notes</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cloudflare incident on March 21, 2025]]></title>
        <id>4I4XNCQlRirlf9SaA9ySTS</id>
        <link href="https://blog.cloudflare.com/cloudflare-incident-march-21-2025/"/>
        <updated>2025-03-25T01:40:38.000Z</updated>
        <summary type="html"><![CDATA[On March 21, 2025, multiple Cloudflare services, including R2 object storage experienced an elevated rate of error responses. Here’s what caused the incident, the impact, and how we are making sure it]]></summary>
        <author>
            <name>Phillip Jones</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[microsoft/playwright-mcp]]></title>
        <id>https://simonwillison.net/2025/Mar/25/playwright-mcp/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/25/playwright-mcp/#atom-everything"/>
        <updated>2025-03-25T01:40:05.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://github.com/microsoft/playwright-mcp">microsoft/playwright-mcp</a></strong></p>
The Playwright team at Microsoft have released an MCP (<a href="https://github.com/microsoft/playwright-mcp">Model Context Protocol</a>) server wrapping Playwright, and it's pretty fascinating.</p>
<p>They implemented it on top of the Chrome accessibility tree, so MCP clients (such as the Claude Desktop app) can use it to drive an automated browser and use the accessibility tree to read and navigate pages that they visit.</p>
<p>Trying it out is quite easy if you have Claude Desktop and Node.js installed already. Edit your <code>claude_desktop_config.json</code> file:</p>
<pre><code>code ~/Library/Application\ Support/Claude/claude_desktop_config.json
</code></pre>
<p>And add this:</p>
<div class="highlight highlight-source-json"><pre>{
  <span class="pl-ent">"mcpServers"</span>: {
    <span class="pl-ent">"playwright"</span>: {
      <span class="pl-ent">"command"</span>: <span class="pl-s"><span class="pl-pds">"</span>npx<span class="pl-pds">"</span></span>,
      <span class="pl-ent">"args"</span>: [
        <span class="pl-s"><span class="pl-pds">"</span>@playwright/mcp@latest<span class="pl-pds">"</span></span>
      ]
    }
  }
}</pre></div>

<p>Now when you launch Claude Desktop various new browser automation tools will be available to it, and you can tell Claude to navigate to a website and interact with it.</p>
<p><img alt="Screenshot of Claude interface showing a conversation about Datasette. The interface shows Claude responding to a user (SW) after navigating to datasette.io. Claude's response includes page details (URL: https://datasette.io/, Title: Datasette: An open source multi-tool for exploring and publishing data) and a summary of what's visible on the site: a description of Datasette as an open-source tool for exploring and publishing data, the tagline &quot;Find stories in data&quot;, navigation options, and features including exploratory data analysis, instant data publishing, and rapid prototyping." src="https://static.simonwillison.net/static/2025/claude-playwright.jpg" /></p>
<p>I ran the following to get a list of the available tools:</p>
<pre><code>cd /tmp
git clone https://github.com/microsoft/playwright-mcp
cd playwright-mcp/src/tools
files-to-prompt . | llm -m claude-3.7-sonnet \
  'Output a detailed description of these tools'
</code></pre>
<p>The <a href="https://gist.github.com/simonw/69200999149221c549c1f62e7befa20f">full output is here</a>, but here's the truncated tool list:</p>
<blockquote>
<h4>Navigation Tools (<code>common.ts</code>)</h4>
<ul>
<li><strong>browser_navigate</strong>: Navigate to a specific URL</li>
<li><strong>browser_go_back</strong>: Navigate back in browser history</li>
<li><strong>browser_go_forward</strong>: Navigate forward in browser history</li>
<li><strong>browser_wait</strong>: Wait for a specified time in seconds</li>
<li><strong>browser_press_key</strong>: Press a keyboard key</li>
<li><strong>browser_save_as_pdf</strong>: Save current page as PDF</li>
<li><strong>browser_close</strong>: Close the current page</li>
</ul>
<h4>Screenshot and Mouse Tools (<code>screenshot.ts</code>)</h4>
<ul>
<li><strong>browser_screenshot</strong>: Take a screenshot of the current page</li>
<li><strong>browser_move_mouse</strong>: Move mouse to specific coordinates</li>
<li><strong>browser_click</strong> (coordinate-based): Click at specific x,y coordinates</li>
<li><strong>browser_drag</strong> (coordinate-based): Drag mouse from one position to another</li>
<li><strong>browser_type</strong> (keyboard): Type text and optionally submit</li>
</ul>
<h4>Accessibility Snapshot Tools (<code>snapshot.ts</code>)</h4>
<ul>
<li><strong>browser_snapshot</strong>: Capture accessibility structure of the page</li>
<li><strong>browser_click</strong> (element-based): Click on a specific element using accessibility reference</li>
<li><strong>browser_drag</strong> (element-based): Drag between two elements</li>
<li><strong>browser_hover</strong>: Hover over an element</li>
<li><strong>browser_type</strong> (element-based): Type text into a specific element</li>
</ul>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/playwright">playwright</a>, <a href="https://simonwillison.net/tags/anthropic">anthropic</a>, <a href="https://simonwillison.net/tags/claude">claude</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/llm-tool-use">llm-tool-use</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/model-context-protocol">model-context-protocol</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Qwen2.5-VL-32B: Smarter and Lighter]]></title>
        <id>https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/24/qwen25-vl-32b/#atom-everything"/>
        <updated>2025-03-24T22:43:22.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/">Qwen2.5-VL-32B: Smarter and Lighter</a></strong></p>
The second big open weight LLM release from China today - the first being <a href="https://simonwillison.net/2025/Mar/24/deepseek/">DeepSeek v3-0324</a>.</p>
<p>Qwen's previous vision model was Qwen2.5 VL, <a href="https://simonwillison.net/2025/Jan/27/qwen25-vl-qwen25-vl-qwen25-vl/">released in January</a> in 3B, 7B and 72B sizes.</p>
<p>Today's Apache 2.0 licensed release is a 32B model, which is quickly becoming my personal favourite model size - large enough to have GPT-4-class capabilities, but small enough that on my 64GB Mac there's still enough RAM for me to run other memory-hungry applications like Firefox and VS Code.</p>
<p>Qwen claim that the new model (when compared to their previous 2.5 VL family) can "align more closely with human preferences", is better at "mathematical reasoning" and provides "enhanced accuracy and detailed analysis in tasks such as image parsing, content recognition, and visual logic deduction".</p>
<p>They also offer some presumably carefully selected benchmark results showing it out-performing Gemma 3-27B, Mistral Small 3.1 24B and GPT-4o-0513 (there have been two more recent GPT-4o releases since that one, 2024-08-16 and 2024-11-20).</p>
<p>As usual, Prince Canuma had MLX versions of the models live within hours of the release, in <a href="https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-4bit">4 bit</a>, <a href="https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-6bit">6 bit</a>, <a href="https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-8bit">8 bit</a>, and <a href="https://huggingface.co/mlx-community/Qwen2.5-VL-32B-Instruct-bf16">bf16</a> variants.</p>
<p>I ran the 4bit version (a 18GB model download) using <code>uv</code> and Prince's <a href="https://github.com/Blaizzy/mlx-vlm">mlx-vlm</a> like this:</p>
<div class="highlight highlight-source-shell"><pre>uv run --with <span class="pl-s"><span class="pl-pds">'</span>numpy&lt;2<span class="pl-pds">'</span></span> --with mlx-vlm \
  python -m mlx_vlm.generate \
    --model mlx-community/Qwen2.5-VL-32B-Instruct-4bit \
    --max-tokens 1000 \
    --temperature 0.0 \
    --prompt <span class="pl-s"><span class="pl-pds">"</span>Describe this image.<span class="pl-pds">"</span></span> \
    --image Mpaboundrycdfw-1.png</pre></div>

<p>Here's the image:</p>
<p><img alt="A map image, described below" src="https://static.simonwillison.net/static/2025/Mpaboundrycdfw-1.png" /></p>
<p>And here's the resulting description, which I found to be <em>very</em> impressive:</p>
<blockquote>
<p>This image appears to be a map highlighting specific areas along the coastline, likely in California, based on the geographical features and place names. Here is a detailed description of the image:</p>
<ol>
<li>
<p><strong>Geographic Features</strong>:</p>
<ul>
<li>The map shows a section of the coastline, with land on the right and the sea on the left.</li>
<li>The land area includes parts of SanMateo County, as noted in the top-right corner.</li>
<li>The coastline features several named locations, including:
<ul>
<li><strong>Point Montara</strong></li>
<li><strong>Pillar Point</strong></li>
<li><strong>Kings Rock</strong></li>
<li><strong>VicenteCreek</strong></li>
<li><strong>DennistonCreek</strong></li>
<li><strong>State Beach</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Highlighted Areas</strong>:</p>
<ul>
<li>The map contains two distinct areas marked with different colors:
<ul>
<li>
<strong>Red Area (Top Section)</strong>
<ul>
<li>Labeled as "MONTARA" and "MONTARA M R" (possibly referring to a "Marine Reserves" or similar protected area).</li>
<li>This area is marked with a dark red color and extends from the coastline inland.</li>
</ul>
</li>
<li>
<strong>Blue Area (Bottom Section)</strong>
<ul>
<li>Labeled as "PILLAR" and "PILLAR P O I N T" along with "SMCA" (Special Management Area).</li>
<li>This area is marked with a blue color and is located closer to the coastline, including the area around "Pillar Point."</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Depth Contours</strong>:</p>
<ul>
<li>The map includes depth markings in the sea, indicating the bathymetrical features:
<ul>
<li>
<strong>20 fathom (fm) line</strong> is marked in the upper-left part of the sea.</li>
<li>
<strong>10 fathom (fm) line</strong> is marked closer to the coastline.</li>
<li>
<strong>30 fathom (fm) line</strong> is marked further out to sea.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Other Features</strong>:</p>
<ul>
<li>
<strong>State Beach</strong>: Marked near the top-right, indicating a protected recreational area.</li>
<li>
<strong>Kings Rock</strong>: A prominent feature near the coastline, likely a rocky outcropping.</li>
<li>
<strong>Creeks</strong>: The map shows several creeks, including <strong>VicenteCreek</strong> and <strong>DennistonCreek</strong>, which flow into the sea.</li>
</ul>
</li>
<li>
<p><strong>Protected Areas</strong>:</p>
<ul>
<li>The map highlights specific protected areas:
<ul>
<li>
<strong>Marine Reserves</strong>:
<ul>
<li>"MONTARA M R" (Marine Reserves) in red.</li>
</ul>
</li>
<li>
<strong>Special Management Area (SMCA)</strong>
<ul>
<li>"PILLAR P O I N T" in blue, indicating a Special Management Area.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Grid and Coordinates</strong>:</p>
<ul>
<li>The map includes a grid with latitude and longitude markings:
<ul>
<li>Latitude ranges from approximately 37°25'N to 37°35'N.</li>
<li>Longitude ranges from approximately 122°22.5'W to 122°35.5'W.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Topography</strong>:</p>
<ul>
<li>The land area shows topographic features, including elevations and vegetation, with green areas indicating higher elevations or vegetated land.</li>
</ul>
</li>
<li>
<p><strong>Other Labels</strong>:</p>
<ul>
<li>
<strong>"SMR"</strong>: Likely stands for "State Managed Reserves."</li>
<li>
<strong>"SMCA"</strong>: Likely stands for "Special Management Control Area."</li>
</ul>
</li>
</ol>
<p>In summary, this map highlights specific protected areas along the coastline, including a red "Marine Reserves" area and a blue "Special Management Area" near "Pillar Point." The map also includes depth markings, geographical features, and place names, providing a detailed view of the region's natural and protected areas.</p>
</blockquote>

<p>It included the following runtime statistics:</p>
<pre><code>Prompt: 1051 tokens, 111.985 tokens-per-sec
Generation: 760 tokens, 17.328 tokens-per-sec
Peak memory: 21.110 GB
</code></pre>


    <p>Tags: <a href="https://simonwillison.net/tags/vision-llms">vision-llms</a>, <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/qwen">qwen</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/uv">uv</a>, <a href="https://simonwillison.net/tags/mlx">mlx</a>, <a href="https://simonwillison.net/tags/prince-canuma">prince-canuma</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Daily Reading List – March 24, 2025 (#518)]]></title>
        <id>http://seroter.com/?p=21705</id>
        <link href="https://seroter.com/2025/03/24/daily-reading-list-march-24-2025-518/"/>
        <updated>2025-03-24T21:44:08.000Z</updated>
        <summary type="html"><![CDATA[Today's links look at how to lead effective engineering teams in an AI world, why you should do discontinuous improvements, and how to develop a more powerful speaking voice.]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AWS Weekly Roundup: Omdia recognition, Amazon Bedrock RAG evaluation, International Women’s Day events, and more (March 24, 2025)]]></title>
        <id>5cfe9f57ec1ec7319ee4820e8e38b07fb764d6ab</id>
        <link href="https://aws.amazon.com/blogs/aws/aws-weekly-roundup-omdia-recognition-amazon-bedrock-rag-evaluation-international-womens-day-events-and-more-march-24-2025/"/>
        <updated>2025-03-24T17:15:01.000Z</updated>
        <summary type="html"><![CDATA[As we celebrate International Women’s Day (IWD) this March, I had the privilege of attending the ‘Women in Tech’ User Group meetup in Shenzhen last weekend. I was inspired to see over 100 women in tech from different industries come together to discuss AI ethics from a female perspective. Together, we explored strategies such as […]]]></summary>
        <author>
            <name>Betty Zheng (郑予彬)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[These are 25 “sure bet” breakout sessions for developers at Google Cloud Next ’25]]></title>
        <id>http://seroter.com/?p=21680</id>
        <link href="https://seroter.com/2025/03/24/these-are-25-sure-bet-breakout-sessions-for-developers-at-google-cloud-next-25/"/>
        <updated>2025-03-24T15:35:13.000Z</updated>
        <summary type="html"><![CDATA[It can feel like a relief to attend a single-track conference. No choices to make!…]]></summary>
        <author>
            <name>Richard Seroter</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[deepseek-ai/DeepSeek-V3-0324]]></title>
        <id>https://simonwillison.net/2025/Mar/24/deepseek/#atom-everything</id>
        <link href="https://simonwillison.net/2025/Mar/24/deepseek/#atom-everything"/>
        <updated>2025-03-24T15:04:04.000Z</updated>
        <summary type="html"><![CDATA[<p><strong><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324">deepseek-ai/DeepSeek-V3-0324</a></strong></p>
Chinese AI lab DeepSeek just released the latest version of their enormous DeepSeek v3 model, baking the release date into the name <code>DeepSeek-V3-0324</code>.</p>
<p>The license is MIT (that's new - previous DeepSeek v3 had a custom license), the README is empty and the release adds up a to a total of 641 GB of files, mostly of the form <code>model-00035-of-000163.safetensors</code>.</p>
<p>The model only came out a few hours ago and MLX developer Awni Hannun already <a href="https://twitter.com/awnihannun/status/1904177084609827054">has it running</a> at &gt;20 tokens/second on a 512GB M3 Ultra Mac Studio ($9,499 of ostensibly consumer-grade hardware) via <a href="https://pypi.org/project/mlx-lm/">mlx-lm</a> and this <a href="https://huggingface.co/mlx-community/DeepSeek-V3-0324-4bit">mlx-community/DeepSeek-V3-0324-4bit</a> 4bit quantization, which reduces the on-disk size to 352 GB.</p>
<p>I think that means if you have that machine you can run it with my <a href="https://github.com/simonw/llm-mlx">llm-mlx</a> plugin like this, but I've not tried myself!</p>
<pre><code>llm mlx download-model mlx-community/DeepSeek-V3-0324-4bit
llm chat -m mlx-community/DeepSeek-V3-0324-4bit
</code></pre>
<p>The new model is also <a href="https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free">listed on OpenRouter</a>. You can try a chat at <a href="https://openrouter.ai/chat?models=deepseek/deepseek-chat-v3-0324:free">openrouter.ai/chat?models=deepseek/deepseek-chat-v3-0324:free</a>.</p>
<p>Here's what the chat interface <a href="https://gist.github.com/simonw/3ce2bf5836743dfaf07d994578b261ba">gave me</a> for "Generate an SVG of a pelican riding a bicycle":</p>
<p><img alt="There's a pelican, and a bicycle, but both of them look disassembled." src="https://static.simonwillison.net/static/2025/deepseek-v3-pelican.jpg" /></p>
<p>I have two API keys with OpenRouter - one of them worked with the model, the other gave me a <code>No endpoints found matching your data policy</code> error - I think because I had a setting on that key disallowing models from training on my activity. The key that worked was a free key with no attached billing credentials.</p>
<p>For my working API key the <a href="https://github.com/simonw/llm-openrouter">llm-openrouter</a> plugin let me run a prompt like this:</p>
<pre><code>llm install llm-openrouter
llm keys set openrouter
# Paste key here
llm -m openrouter/deepseek/deepseek-chat-v3-0324:free "best fact about a pelican"
</code></pre>
<p>Here's that "best fact" - the terminal output included Markdown and an emoji combo, here that's rendered.</p>
<blockquote>
<p>One of the most fascinating facts about pelicans is their <strong>unique throat pouch</strong>, called a <strong>gular sac</strong>, which can hold up to <strong>3 gallons (11 liters) of water</strong>—three times more than their stomach!  </p>
<p>Here’s why it’s amazing:<br />
- <strong>Fishing Tool</strong>: They use it like a net to scoop up fish, then drain the water before swallowing.<br />
- <strong>Cooling Mechanism</strong>: On hot days, pelicans flutter the pouch to stay cool by evaporating water.<br />
- <strong>Built-in "Shopping Cart"</strong>: Some species even use it to carry food back to their chicks.  </p>
<p>Bonus fact: Pelicans often <strong>fish cooperatively</strong>, herding fish into shallow water for an easy catch.  </p>
<p>Would you like more cool pelican facts? 🐦🌊</p>
</blockquote>
<p>In putting this post together I <a href="https://claude.ai/share/fc65bf9b-ae2d-4b23-bd09-ed0d54ff4b56">got Claude</a> to build me <a href="https://tools.simonwillison.net/huggingface-storage">this new tool</a> for finding the total on-disk size of a Hugging Face repository, which is available in their API but not currently displayed on their website.</p>
<p><strong>Update</strong>: Here's a notable independent benchmark <a href="https://twitter.com/paulgauthier/status/1904304052500148423">from Paul Gauthier</a>:</p>
<blockquote>
<p>DeepSeek's new V3 scored 55% on aider's <a href="https://aider.chat/docs/leaderboards/">polyglot benchmark</a>, significantly improving over the prior version. It's the #2 non-thinking/reasoning model, behind only Sonnet 3.7. V3 is competitive with thinking models like R1 &amp; o3-mini.</p>
</blockquote>


    <p>Tags: <a href="https://simonwillison.net/tags/llm-release">llm-release</a>, <a href="https://simonwillison.net/tags/hugging-face">hugging-face</a>, <a href="https://simonwillison.net/tags/generative-ai">generative-ai</a>, <a href="https://simonwillison.net/tags/deepseek">deepseek</a>, <a href="https://simonwillison.net/tags/ai">ai</a>, <a href="https://simonwillison.net/tags/llms">llms</a>, <a href="https://simonwillison.net/tags/mlx">mlx</a>, <a href="https://simonwillison.net/tags/llm">llm</a>, <a href="https://simonwillison.net/tags/ai-assisted-programming">ai-assisted-programming</a>, <a href="https://simonwillison.net/tags/tools">tools</a>, <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle">pelican-riding-a-bicycle</a></p>]]></summary>
        <author>
            <name>Simon Willison's Weblog</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Week 2025: in review]]></title>
        <id>57bTt5UYhnjdF2MwuEePqb</id>
        <link href="https://blog.cloudflare.com/security-week-2025-wrap-up/"/>
        <updated>2025-03-24T13:05:00.000Z</updated>
        <summary type="html"><![CDATA[Security Week 2025 has officially come to a close. Our updates for the week included a deep dive on our AI offering, a unified navigation experience, and an introduction to our AI Agent Cloudy.]]></summary>
        <author>
            <name>Kim Blight</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New URLPattern API brings improved pattern matching to Node.js and Cloudflare Workers]]></title>
        <id>55t98SAXi3erhs7Wn5dgno</id>
        <link href="https://blog.cloudflare.com/improving-web-standards-urlpattern/"/>
        <updated>2025-03-24T13:00:00.000Z</updated>
        <summary type="html"><![CDATA[Today we're announcing our latest contribution to Node.js, now available in v23.8.0: URLPattern.]]></summary>
        <author>
            <name>Yagiz Nizipli</name>
        </author>
    </entry>
</feed>